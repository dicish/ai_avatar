import os
import torch
from gradio_client import Client
import streamlit as st
import time
import cv2
from PIL import Image


text = st.text_input("–í–≤–µ–¥–∏—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è –æ–∑–≤—É—á–∫–∏:")


def validate_image(image_path):
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
    try:
        img = cv2.imread(image_path)
        if img is None:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ")

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ RGB, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if len(img.shape) == 2:  # –ì—Ä–∞–¥–∞—Ü–∏–∏ —Å–µ—Ä–æ–≥–æ
            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)
        elif img.shape[2] == 4:  # –° –∞–ª—å—Ñ–∞-–∫–∞–Ω–∞–ª–æ–º
            img = cv2.cvtColor(img, cv2.COLOR_BGRA2RGB)
        else:
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        fixed_path = os.path.join(os.path.dirname(image_path), "fixed_" + os.path.basename(image_path))
        Image.fromarray(img).save(fixed_path)
        return fixed_path
    except Exception as e:
        raise RuntimeError(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {str(e)}")


def text_to_speech(text, output_path='output.wav'):
    language = 'ru'
    model_id = 'v3_1_ru'
    speaker = 'baya'

    model, _ = torch.hub.load(
        repo_or_dir='snakers4/silero-models',
        model='silero_tts',
        language=language,
        speaker=model_id
    )

    model.save_wav(
        text=text,
        speaker=speaker,
        sample_rate=48000,
        audio_path=output_path
    )
    return os.path.abspath(output_path)


def animate_with_sadtalker(image_path, audio_path, server_url="http://127.0.0.1:7860/"):
    client = Client(server_url)
    time.sleep(10)  # –ñ–¥—ë–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–µ—Ä–≤–µ—Ä–∞

    try:
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã SadTalker
        result = client.predict(
            image_path,
            audio_path,
            "full",
            False,
            True,
            1,
            256,
            0,
            fn_index=0
        )
        return result
    except:
        print("‚è≥ 2 –º–∏–Ω—É—Ç—ã –∏ –≤–∞—à–µ –≤–∏–¥–µ–æ –ø–æ—è–≤–∏—Ç—Å—è –≤ results")
        return None


if text:

    try:

        st.info("üéô –°–æ–∑–¥–∞–Ω–∏–µ –∞—É–¥–∏–æ...")

        audio_file = text_to_speech(text)

        st.info(f"‚úÖ –ê—É–¥–∏–æ —Å–æ–∑–¥–∞–Ω–æ: {audio_file}")

        image_file = os.path.abspath('test.jpg')

        if not os.path.exists(image_file):
            raise FileNotFoundError(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {image_file}")

        st.info("üñº –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")

        fixed_image = validate_image(image_file)

        st.info(f"‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {fixed_image}")

        st.info("üéû –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–∏–º–∞—Ü–∏–∏...")

        result_video = animate_with_sadtalker(fixed_image, audio_file)

        if result_video:

            st.info(f"üé¨ –ì–æ—Ç–æ–≤–æ! –í–∏–¥–µ–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {result_video}")

        else:

            st.info("‚ÑπÔ∏è –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–∞–ø–∫—É results ‚Äî –≤–∏–¥–µ–æ –ø–æ—è–≤–∏—Ç—Å—è —Ç–∞–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.")


    except Exception as e:

        st.info(f"‚ùå –û—à–∏–±–∫–∞: {str(e)}")
